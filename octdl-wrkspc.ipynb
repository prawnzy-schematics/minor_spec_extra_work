{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"aec1394341b149d2ac6f5024afab5b12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7cc75f166b6948f78396942f25b8bf04","IPY_MODEL_fcd4d7cdfcfc44678221956ed98eb698","IPY_MODEL_239e37bd05d54a9f8ba65887412ef0ea"],"layout":"IPY_MODEL_ebb94120e7fb42cb8d24429e602b9091"}},"7cc75f166b6948f78396942f25b8bf04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c41df2cc591545bbb91a244428f11406","placeholder":"​","style":"IPY_MODEL_53865a3bf18a4fb1a3f845c245e7bcdc","value":"model.safetensors: 100%"}},"fcd4d7cdfcfc44678221956ed98eb698":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f8ea1cd2aab42b3b4551d5463eb3afc","max":346284714,"min":0,"orientation":"horizontal","style":"IPY_MODEL_612da653545c44b68d069543240639bd","value":346284714}},"239e37bd05d54a9f8ba65887412ef0ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9fffa3f1aa14724a64cc13eb2c839e1","placeholder":"​","style":"IPY_MODEL_1d098e01e7f443ef8c3a243bb0db0d52","value":" 346M/346M [00:05&lt;00:00, 33.3MB/s]"}},"ebb94120e7fb42cb8d24429e602b9091":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c41df2cc591545bbb91a244428f11406":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53865a3bf18a4fb1a3f845c245e7bcdc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f8ea1cd2aab42b3b4551d5463eb3afc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"612da653545c44b68d069543240639bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9fffa3f1aa14724a64cc13eb2c839e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d098e01e7f443ef8c3a243bb0db0d52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14128264,"sourceType":"datasetVersion","datasetId":9001975}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"zip_path = \"/content/OCTDL.zip\"     # <-- update if needed\nextract_dir = \"/content/OCTDL\"      # destination folder\n","metadata":{"id":"fPLfILvpKpQ4","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:25:38.181867Z","iopub.execute_input":"2025-12-13T08:25:38.182663Z","iopub.status.idle":"2025-12-13T08:25:38.186488Z","shell.execute_reply.started":"2025-12-13T08:25:38.182632Z","shell.execute_reply":"2025-12-13T08:25:38.185687Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!unzip -q \"$zip_path\" -d \"$extract_dir\"\n","metadata":{"id":"2bPVzbRxV9GC","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:25:38.187814Z","iopub.execute_input":"2025-12-13T08:25:38.188130Z","iopub.status.idle":"2025-12-13T08:25:38.347739Z","shell.execute_reply.started":"2025-12-13T08:25:38.188110Z","shell.execute_reply":"2025-12-13T08:25:38.347004Z"}},"outputs":[{"name":"stdout","text":"unzip:  cannot find or open /content/OCTDL.zip, /content/OCTDL.zip.zip or /content/OCTDL.zip.ZIP.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\n\nroot = \"/kaggle/input/octdl-dataset\"    # update if your extract dir is different\nprint(\"Folders inside OCTDL:\", os.listdir(root))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mSM7IYaWAz_","outputId":"9cf98e97-42d7-4c2f-f3e9-fa812c2255f7","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:25:38.348623Z","iopub.execute_input":"2025-12-13T08:25:38.348825Z","iopub.status.idle":"2025-12-13T08:25:38.368941Z","shell.execute_reply.started":"2025-12-13T08:25:38.348803Z","shell.execute_reply":"2025-12-13T08:25:38.368381Z"}},"outputs":[{"name":"stdout","text":"Folders inside OCTDL: ['OCTDL']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"id":"ykSsU-IWXT8F","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define the paths\nroot_dir = \"/kaggle/input\"\nnested_dir = os.path.join(root_dir, \"OCTDL\")\n\nif os.path.exists(nested_dir):\n    # Move contents from nested_dir to root_dir\n    for item in os.listdir(nested_dir):\n        s = os.path.join(nested_dir, item)\n        d = os.path.join(root_dir, item)\n        if os.path.isdir(s):\n            shutil.move(s, d)\n        else:\n            shutil.move(s, d)\n\n    # Remove the now empty nested_dir\n    shutil.rmtree(nested_dir)\n    print(f\"Flattened directory structure: moved contents from '{nested_dir}' to '{root_dir}' and removed '{nested_dir}'\")\nelse:\n    print(f\"No nested directory '{nested_dir}' found to flatten.\")\n\nprint(\"Current folders inside OCTDL:\", os.listdir(root_dir))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1df2b475","outputId":"a3f11d1c-5573-45c5-e914-fa4ed6f0c866","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:25:38.370438Z","iopub.execute_input":"2025-12-13T08:25:38.370816Z","iopub.status.idle":"2025-12-13T08:25:38.376762Z","shell.execute_reply.started":"2025-12-13T08:25:38.370798Z","shell.execute_reply":"2025-12-13T08:25:38.376054Z"}},"outputs":[{"name":"stdout","text":"No nested directory '/kaggle/input/OCTDL' found to flatten.\nCurrent folders inside OCTDL: ['octdl-dataset']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\n\nroot = \"/kaggle/input/octdl-dataset/OCTDL\"    # update if your extract dir is different\nprint(\"Folders inside OCTDL:\", os.listdir(root))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6135f330","outputId":"1869b5a4-c9f7-4483-cbeb-53297006f1aa","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:25:38.377756Z","iopub.execute_input":"2025-12-13T08:25:38.378032Z","iopub.status.idle":"2025-12-13T08:25:38.394715Z","shell.execute_reply.started":"2025-12-13T08:25:38.378009Z","shell.execute_reply":"2025-12-13T08:25:38.394059Z"}},"outputs":[{"name":"stdout","text":"Folders inside OCTDL: ['NO', 'AMD', 'VID', 'ERM', 'RVO', 'DME', 'RAO']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# === OCTDL: Optimized Dataset + Dataloader Setup (single cell) ===\n# - Designed for Colab T4, persistent_workers=True for speed.\n\nimport os, json, time, random\nfrom pathlib import Path\nfrom collections import Counter\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\n# --------------------- CONFIG ---------------------\nROOT = \"/kaggle/input/octdl-dataset/OCTDL\"            # <- update if folder is in a different place\nIMG_SIZE = 224\nBATCH_SIZE = 32                    # target per-GPU batch for ViT/DeiT; lower for MaxViT if OOM\nNUM_WORKERS = 0                    # Colab T4 sweet spot: 2-4\nPIN_MEMORY = True\nPERSISTENT_WORKERS = True\nVAL_SPLIT = 0.20                   # stratified val split fraction\nRANDOM_SEED = 42\nCLASS_MAP_JSON = \"/content/class_to_idx.json\"\nTHROUGHPUT_BATCHES = 20            # number of batches to sample for a quick throughput test\n\n# Repro\ntorch.manual_seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\n\n# --------------------- Sanity: dataset root & classes ---------------------\nroot_path = Path(ROOT)\nassert root_path.exists(), f\"Dataset root not found at {ROOT}. Update ROOT and re-run.\"\n\nclasses = [p.name for p in sorted(root_path.iterdir()) if p.is_dir()]\nassert len(classes) > 0, \"No class folders found — ensure dataset follows ImageFolder layout.\"\nprint(\"Detected classes (sorted):\", classes)\n\n# --------------------- Gather samples and per-class counts ---------------------\nIMG_EXTS = (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\")\nclass_to_idx = {cls: i for i, cls in enumerate(classes)}\nall_samples = []\ncounts = Counter()\n\nfor cls in classes:\n    cls_dir = root_path / cls\n    files = [p for p in sorted(cls_dir.iterdir()) if p.suffix.lower() in IMG_EXTS]\n    counts[cls] = len(files)\n    for p in files:\n        all_samples.append((str(p), class_to_idx[cls]))\n\ntotal_images = len(all_samples)\nprint(f\"Total images detected: {total_images}\")\nprint(\"Per-class counts:\")\nfor k,v in counts.items():\n    print(f\"  {k}: {v}\")\n\n# Save class map (ensures reproducible label ordering)\nwith open(CLASS_MAP_JSON, \"w\") as f:\n    json.dump(class_to_idx, f, indent=2)\nprint(f\"Saved class->index mapping to: {CLASS_MAP_JSON}\")\n\n# --------------------- Stratified split (no sklearn dependency) ---------------------\nrandom.shuffle(all_samples)  # shuffle globally first\ntrain_items, val_items = [], []\nper_class_files = {}\nfor p, label in all_samples:\n    per_class_files.setdefault(label, []).append(p)\n\nfor label, files in per_class_files.items():\n    random.shuffle(files)\n    n_val = max(1, int(len(files) * VAL_SPLIT))\n    val_files = files[:n_val]\n    train_files = files[n_val:]\n    # ensure at least one train item per class if very small:\n    if len(train_files) == 0 and len(val_files) > 1:\n        train_files.append(val_files.pop())\n    train_items += [(p, label) for p in train_files]\n    val_items   += [(p, label) for p in val_files]\n\nprint(f\"Train samples: {len(train_items)}  |  Val samples: {len(val_items)}\")\n\n# --------------------- Transforms ---------------------\n# Use ImageNet normalization for timm pretrained backbones\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD  = [0.229, 0.224, 0.225]\n\ntrain_tf = transforms.Compose([\n    # convert grayscale to RGB if needed; if already RGB this is no-op\n    transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=10),\n    transforms.ColorJitter(brightness=0.08, contrast=0.08),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n])\n\nval_tf = transforms.Compose([\n    transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n])\n\n# --------------------- Dataset wrapper from (path,label) pairs ---------------------\nclass PathListDataset(Dataset):\n    def __init__(self, items, transform=None):\n        self.items = items\n        self.transform = transform\n    def __len__(self):\n        return len(self.items)\n    def __getitem__(self, idx):\n        p, label = self.items[idx]\n        img = Image.open(p)\n        # For safety, convert to RGB here so transforms get consistent PIL mode\n        if img.mode != \"RGB\":\n            img = img.convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\ntrain_ds = PathListDataset(train_items, transform=train_tf)\nval_ds   = PathListDataset(val_items, transform=val_tf)\n\n# --------------------- DataLoaders ---------------------\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n     prefetch_factor=None\n)\nval_loader = DataLoader(\n    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=max(1, NUM_WORKERS//2), pin_memory=PIN_MEMORY,\n    prefetch_factor=None\n)\n\n# --------------------- Quick sanity checks ---------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# one-batch shape check\ntry:\n    imgs, labels = next(iter(train_loader))\n    print(\"One batch imgs.shape:\", imgs.shape, \"  labels.shape:\", labels.shape)\n    assert imgs.ndim == 4 and imgs.shape[1] == 3, \"Expected [B,3,H,W] tensors\"\n    print(\"Label distribution in this batch:\", dict(Counter(labels.tolist())))\nexcept Exception as e:\n    print(\"ERROR while fetching batch (check transforms / disk read):\", e)\n\n# throughput test (data + transforms only) over a few batches\nprint(\"\\nRunning a quick throughput test (data + transforms)...\")\nstart = time.time()\nn = 0\nfor i, (x, y) in enumerate(train_loader):\n    n += x.size(0)\n    if i+1 >= THROUGHPUT_BATCHES:\n        break\nelapsed = time.time() - start\nprint(f\"Loaded {n} images in {elapsed:.2f}s -> {n/elapsed:.2f} samples/sec (approx)\")\n\n# print sample entries and class map\nprint(\"\\nSample train items (5):\")\nfor p,l in train_items[:5]:\n    print(\" \", p, \"->\", [k for k,v in class_to_idx.items() if v==l][0])\n\nprint(\"\\nclass_to_idx mapping saved to:\", CLASS_MAP_JSON)\nwith open(CLASS_MAP_JSON, \"w\") as f:\n    json.dump(class_to_idx, f, indent=2)\n\nprint(\"\\nDataloaders ready — proceed to model creation & training loop.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BqXlOrXUZemi","outputId":"4b08adeb-48c4-4414-93c8-016b7912258c","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:25:38.395591Z","iopub.execute_input":"2025-12-13T08:25:38.395925Z","iopub.status.idle":"2025-12-13T08:25:46.940795Z","shell.execute_reply.started":"2025-12-13T08:25:38.395902Z","shell.execute_reply":"2025-12-13T08:25:46.940046Z"}},"outputs":[{"name":"stdout","text":"Detected classes (sorted): ['AMD', 'DME', 'ERM', 'NO', 'RAO', 'RVO', 'VID']\nTotal images detected: 1618\nPer-class counts:\n  AMD: 885\n  DME: 143\n  ERM: 133\n  NO: 284\n  RAO: 22\n  RVO: 93\n  VID: 58\nSaved class->index mapping to: /content/class_to_idx.json\nTrain samples: 1298  |  Val samples: 320\nDevice: cuda\nOne batch imgs.shape: torch.Size([32, 3, 224, 224])   labels.shape: torch.Size([32])\nLabel distribution in this batch: {0: 18, 3: 7, 1: 5, 6: 1, 2: 1}\n\nRunning a quick throughput test (data + transforms)...\nLoaded 640 images in 6.49s -> 98.60 samples/sec (approx)\n\nSample train items (5):\n  /kaggle/input/octdl-dataset/OCTDL/AMD/amd_e_108.jpg -> AMD\n  /kaggle/input/octdl-dataset/OCTDL/AMD/amd_l_149.jpg -> AMD\n  /kaggle/input/octdl-dataset/OCTDL/AMD/amd_f_91.jpg -> AMD\n  /kaggle/input/octdl-dataset/OCTDL/AMD/amd_e_30.jpg -> AMD\n  /kaggle/input/octdl-dataset/OCTDL/AMD/amd_l_37.jpg -> AMD\n\nclass_to_idx mapping saved to: /content/class_to_idx.json\n\nDataloaders ready — proceed to model creation & training loop.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# === Finalize class-imbalance: compute class weights and create weighted criterion ===\n# Place this cell before your training cell. It will compute normalized inverse-frequency\n# weights and create `criterion` (CrossEntropyLoss) that you can use directly in training.\n#\n# Requirements: train_items (list of (path,label)) OR train_loader available from previous cells.\n# It will save weights to /content/class_weights.json for reproducibility.\n\nimport os, json\nfrom collections import Counter\nimport torch\nimport math\n\n# ----- Config: label smoothing you used previously (adjust if needed) -----\nLABEL_SMOOTHING = 0.1  # keep same training logic as before\n\n# ----- Helper: get per-class counts (robust) -----\ndef get_train_counts():\n    # 1) Preferred: train_items list created by the split cell (list of (path,label))\n    if 'train_items' in globals() and isinstance(train_items, (list,tuple)) and len(train_items)>0:\n        labels = [lbl for _, lbl in train_items]\n        return Counter(labels)\n    # 2) If you built a train_loader / train_ds earlier\n    if 'train_loader' in globals():\n        ds = train_loader.dataset\n        if hasattr(ds, \"items\"):                  # our PathListDataset\n            return Counter([lbl for _, lbl in ds.items])\n        if hasattr(ds, \"targets\"):                # torchvision ImageFolder\n            return Counter(ds.targets)\n        if hasattr(ds, \"imgs\"):                   # ImageFolder: .imgs = [(path,label),...]\n            return Counter([lbl for _, lbl in ds.imgs])\n    # 3) Fallback: read saved class_to_idx and count files on disk (last resort)\n    try:\n        with open('/content/class_to_idx.json','r') as f:\n            class_to_idx = json.load(f)\n        counts = Counter()\n        for cls, idx in class_to_idx.items():\n            folder = os.path.join('/content/OCTDL', cls)\n            if os.path.isdir(folder):\n                files = [fn for fn in os.listdir(folder) if fn.lower().endswith(('.png','.jpg','.jpeg','.tif','.tiff','.bmp'))]\n                counts[idx] = len(files)\n        if sum(counts.values())>0:\n            return counts\n    except Exception:\n        pass\n    raise RuntimeError(\"Could not infer train counts. Ensure train_items or train_loader.dataset exist or class_to_idx.json and folders are present.\")\n\ncounts = get_train_counts()\nnum_classes = len(counts)\nprint(\"Per-class counts (label_idx -> count):\")\nfor k in sorted(counts.keys()):\n    print(f\"  {k}: {counts[k]}\")\n\n# ----- Compute inverse-frequency weights (normalized) -----\ninv_freq = [0.0] * num_classes\nfor i in range(num_classes):\n    inv_freq[i] = 1.0 / max(1, counts.get(i, 1))\n\nweights = torch.tensor(inv_freq, dtype=torch.float)\n# Normalize so average weight ~1 (keeps loss scale comparable to unweighted CE)\nweights = weights / weights.mean()\n\n# Save weights for reproducibility\nweights_path = \"/content/class_weights.json\"\nwith open(weights_path, \"w\") as f:\n    json.dump({\"weights\": weights.tolist(), \"counts\": dict(counts)}, f, indent=2)\nprint(f\"\\nSaved class weights to {weights_path}\")\nprint(\"Normalized class weights:\", weights.tolist())\n\n# ----- Create criterion (move to device inside training if you prefer) -----\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncriterion = torch.nn.CrossEntropyLoss(weight=weights.to(device), label_smoothing=LABEL_SMOOTHING)\nprint(f\"Criterion created: CrossEntropyLoss with label_smoothing={LABEL_SMOOTHING} and device={device}\")\n\n# Expose for downstream cells\nglobals()['class_weights_tensor'] = weights\nglobals()['criterion'] = criterion\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7N-fDdBhJOl","outputId":"c55c6027-fc28-47d3-fa08-96eba80b95f3","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:25:46.941848Z","iopub.execute_input":"2025-12-13T08:25:46.942326Z","iopub.status.idle":"2025-12-13T08:25:46.984723Z","shell.execute_reply.started":"2025-12-13T08:25:46.942297Z","shell.execute_reply":"2025-12-13T08:25:46.984028Z"}},"outputs":[{"name":"stdout","text":"Per-class counts (label_idx -> count):\n  0: 708\n  1: 115\n  2: 107\n  3: 228\n  4: 18\n  5: 75\n  6: 47\n\nSaved class weights to /content/class_weights.json\nNormalized class weights: [0.08672407269477844, 0.5339186191558838, 0.5738378167152405, 0.26930108666419983, 3.411147117614746, 0.8186752796173096, 1.306396722793579]\nCriterion created: CrossEntropyLoss with label_smoothing=0.1 and device=cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# === Sanity training run (FINAL, robust, Kaggle-safe) ===\n# - Assumes train_loader and val_loader are ALREADY DEFINED\n# - Works with PathListDataset / ImageFolder / Subset\n# - No reliance on dataset internals (.classes, .dataset, json files)\n# - AMP (new API), grad clipping, warmup + cosine LR\n# - num_workers = 0 compatible\n# - Short run by default (epochs=2)\n\nimport os, time, math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport timm\nimport torch.optim as optim\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\n\n# ---------------- CFG ----------------\nCFG = {\n    \"model_name\": \"deit_base_patch16_224\",\n    \"pretrained\": True,\n    \"epochs\": 2,                # sanity run\n    \"base_lr\": 1e-4,\n    \"weight_decay\": 1e-2,\n    \"accum_steps\": 1,\n    \"grad_clip\": 1.0,\n    \"warmup_pct\": 0.05,\n    \"save_dir\": \"/kaggle/working/checkpoints_sanity\",\n    \"log_every\": 50,\n}\nos.makedirs(CFG[\"save_dir\"], exist_ok=True)\n\n# ---------------- Device ----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ---------------- Infer num_classes robustly ----------------\ndef infer_num_classes(ds):\n    labels = set()\n    for i in range(len(ds)):\n        _, y = ds[i]\n        labels.add(int(y))\n    labels = sorted(labels)\n    return labels, len(labels)\n\nlabel_set, num_classes = infer_num_classes(train_loader.dataset)\nprint(\"Detected label indices:\", label_set)\nprint(\"num_classes =\", num_classes)\n\n# ---------------- Model ----------------\nprint(\"Creating model:\", CFG[\"model_name\"])\nmodel = timm.create_model(\n    CFG[\"model_name\"],\n    pretrained=CFG[\"pretrained\"],\n    num_classes=num_classes\n).to(device)\n\n# ---------------- Optimizer & Scheduler ----------------\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=CFG[\"base_lr\"],\n    weight_decay=CFG[\"weight_decay\"]\n)\n\nsteps_per_epoch = len(train_loader)\ntotal_steps = steps_per_epoch * CFG[\"epochs\"]\nwarmup_steps = int(total_steps * CFG[\"warmup_pct\"])\n\ndef lr_lambda(step):\n    if step < warmup_steps:\n        return step / max(1, warmup_steps)\n    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n    return 0.5 * (1.0 + math.cos(math.pi * progress))\n\nscheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n# ---------------- Loss ----------------\n# Uses existing class-weighted criterion if present\nif \"criterion\" not in globals():\n    print(\"Criterion not found — using unweighted CE (fallback)\")\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\nelse:\n    criterion = globals()[\"criterion\"]\n\n# ---------------- AMP Scaler (NEW API) ----------------\nscaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n\n# ---------------- Validation ----------------\ndef validate(model, loader, criterion):\n    model.eval()\n    losses, all_probs, all_labels = [], [], []\n\n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs = imgs.to(device)\n            labels = labels.to(device)\n\n            with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n                out = model(imgs)\n                loss = criterion(out, labels)\n\n            losses.append(loss.item() * imgs.size(0))\n            all_probs.append(torch.softmax(out, dim=1).cpu().numpy())\n            all_labels.append(labels.cpu().numpy())\n\n    all_probs = np.concatenate(all_probs, axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n    preds = all_probs.argmax(axis=1)\n\n    avg_loss = sum(losses) / len(all_labels)\n    acc = (preds == all_labels).mean()\n\n    try:\n        auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovr\")\n    except Exception:\n        auc = float(\"nan\")\n\n    cm = confusion_matrix(all_labels, preds)\n    return avg_loss, acc, auc, cm\n\n# ---------------- Training Loop ----------------\nbest_auc = -1.0\n\nfor epoch in range(1, CFG[\"epochs\"] + 1):\n    model.train()\n    optimizer.zero_grad()\n\n    running_loss, correct, total = 0.0, 0, 0\n    epoch_start = time.time()\n\n    for step, (imgs, labels) in enumerate(train_loader):\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n            out = model(imgs)\n            loss = criterion(out, labels) / CFG[\"accum_steps\"]\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % CFG[\"accum_steps\"] == 0 or (step + 1) == len(train_loader):\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CFG[\"grad_clip\"])\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n\n        running_loss += loss.item() * CFG[\"accum_steps\"] * imgs.size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n\n        if (step + 1) % CFG[\"log_every\"] == 0:\n            print(\n                f\"Epoch {epoch} | step {step+1}/{len(train_loader)} \"\n                f\"| loss {running_loss/total:.4f} | acc {correct/total:.4f}\"\n            )\n\n    train_loss = running_loss / total\n    train_acc = correct / total\n    val_loss, val_acc, val_auc, cm = validate(model, val_loader, criterion)\n    epoch_time = time.time() - epoch_start\n\n    print(\n        f\"\\nEpoch {epoch} summary: \"\n        f\"train_loss {train_loss:.4f} train_acc {train_acc:.4f} | \"\n        f\"val_loss {val_loss:.4f} val_acc {val_acc:.4f} val_auc {val_auc:.4f} | \"\n        f\"time {epoch_time:.1f}s\"\n    )\n    print(\"Confusion matrix:\")\n    print(cm)\n\n    ckpt = {\n        \"epoch\": epoch,\n        \"model_state\": model.state_dict(),\n        \"optim_state\": optimizer.state_dict(),\n        \"val_auc\": val_auc,\n    }\n\n    torch.save(ckpt, os.path.join(CFG[\"save_dir\"], f\"last_{CFG['model_name']}.pth\"))\n    if not math.isnan(val_auc) and val_auc > best_auc:\n        best_auc = val_auc\n        torch.save(ckpt, os.path.join(CFG[\"save_dir\"], f\"best_{CFG['model_name']}.pth\"))\n        print(\"Saved new best checkpoint.\")\n\nprint(f\"\\nSanity run finished. Best val AUC: {best_auc:.4f}\")\nprint(f\"Checkpoints saved in: {CFG['save_dir']}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":788,"referenced_widgets":["aec1394341b149d2ac6f5024afab5b12","7cc75f166b6948f78396942f25b8bf04","fcd4d7cdfcfc44678221956ed98eb698","239e37bd05d54a9f8ba65887412ef0ea","ebb94120e7fb42cb8d24429e602b9091","c41df2cc591545bbb91a244428f11406","53865a3bf18a4fb1a3f845c245e7bcdc","7f8ea1cd2aab42b3b4551d5463eb3afc","612da653545c44b68d069543240639bd","d9fffa3f1aa14724a64cc13eb2c839e1","1d098e01e7f443ef8c3a243bb0db0d52"]},"id":"SBu4GFA0h-SR","outputId":"d810a69b-f5ae-4bee-a347-64c74f55e5f3","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:25:46.985499Z","iopub.execute_input":"2025-12-13T08:25:46.985699Z","iopub.status.idle":"2025-12-13T08:27:00.197731Z","shell.execute_reply.started":"2025-12-13T08:25:46.985682Z","shell.execute_reply":"2025-12-13T08:27:00.196462Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\nDetected label indices: [0, 1, 2, 3, 4, 5, 6]\nnum_classes = 7\nCreating model: vit_base_patch16_224\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 summary: train_loss 2.4869 train_acc 0.0755 | val_loss 2.8900 val_acc 0.1656 val_auc 0.8210 | time 26.0s\nConfusion matrix:\n[[  0   2   0  12 161   1   1]\n [  0   6   0   1  21   0   0]\n [  0   1   3   6  16   0   0]\n [  0   0   0  40  16   0   0]\n [  0   0   0   0   4   0   0]\n [  0   0   0   3  15   0   0]\n [  0   0   0   4   7   0   0]]\nSaved new best checkpoint.\n\nEpoch 2 summary: train_loss 1.8608 train_acc 0.5593 | val_loss 2.5436 val_acc 0.6281 val_auc nan | time 25.5s\nConfusion matrix:\n[[106   1   6  21  22  19   2]\n [  0  17   3   0   0   8   0]\n [  1   0  17   6   1   1   0]\n [  0   0   8  46   2   0   0]\n [  0   0   0   0   4   0   0]\n [  1   0   6   1   0   9   1]\n [  1   0   3   5   0   0   2]]\n\nSanity run finished. Best val AUC: 0.8210\nCheckpoints saved in: /kaggle/working/checkpoints_sanity\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"class-weighted loss helped stability, but rare-classes can still be underexposed. Next logical step: try oversampling (WeightedRandomSampler) while keeping class weights as a fallback. Also add targeted augmentation for minority classes and/or try focal loss if oversampling overfits.","metadata":{"id":"opthw6T3jKzC"}},{"cell_type":"code","source":"# === Replace train_loader with WeightedRandomSampler (oversampling) ===\nfrom collections import Counter\nimport torch\nfrom torch.utils.data import WeightedRandomSampler, DataLoader\n\n# CONFIG\nOVERSAMPLE_MULTIPLIER = 1.0   # try 1.0 first; increase to 1.5 or 2.0 if still underexposed\nREPLACEMENT = True\nBATCH_SIZE = getattr(train_loader, \"batch_size\", 32)\nNUM_WORKERS = getattr(train_loader, \"num_workers\", 0)\nPIN_MEMORY = getattr(train_loader, \"pin_memory\", True)\nPERSISTENT = getattr(train_loader, \"persistent_workers\", True)\n\n# get per-sample labels in train order (train_items from earlier split)\nif 'train_items' in globals() and len(train_items)>0:\n    sample_labels = [lbl for _, lbl in train_items]\nelse:\n    # fallback: try to read dataset attribute\n    ds = train_loader.dataset\n    if hasattr(ds, \"items\"):\n        sample_labels = [lbl for _, lbl in ds.items]\n    elif hasattr(ds, \"targets\"):\n        sample_labels = list(ds.targets)\n    elif hasattr(ds, \"imgs\"):\n        sample_labels = [label for _, label in ds.imgs]\n    else:\n        raise RuntimeError(\"No train_items or dataset label list found. Ensure train split exists.\")\n\n# compute counts (label -> freq)\ncounts = Counter(sample_labels)\nprint(\"Train label counts (before sampler):\", dict(counts))\n\n# per-sample weight = 1 / class_count[label]\nsample_weights = [1.0 / counts[int(l)] for l in sample_labels]\nsample_weights_tensor = torch.DoubleTensor(sample_weights)\n\n# compute num_samples for one epoch\nnum_samples = int(len(sample_labels) * OVERSAMPLE_MULTIPLIER)\nsampler = WeightedRandomSampler(weights=sample_weights_tensor, num_samples=num_samples, replacement=REPLACEMENT)\n\n# create new train_loader that uses the sampler (disable shuffle)\nnew_train_loader = DataLoader(\n    train_loader.dataset,\n    batch_size=BATCH_SIZE,\n    sampler=sampler,\n    num_workers=NUM_WORKERS,\n    pin_memory=PIN_MEMORY,\n    persistent_workers=PERSISTENT,\n    prefetch_factor=None\n)\n\n# Quick check: show label distribution over first N batches\nfrom collections import Counter\ntmp_cnt = Counter()\nfor i, (_, lbls) in enumerate(new_train_loader):\n    tmp_cnt.update(lbls.tolist())\n    if i >= 30:   # sample ~30 batches\n        break\nprint(\"Sampler-sampled label counts (approx over ~30 batches):\", dict(tmp_cnt))\n\n# Replace global train_loader\ntrain_loader = new_train_loader\nprint(\"train_loader replaced with WeightedRandomSampler. num_samples per epoch:\", num_samples)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJvETCIOjMsr","outputId":"a9bceb1b-519b-46e2-ac08-b639468fcfd2","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:27:00.200349Z","iopub.execute_input":"2025-12-13T08:27:00.200641Z","iopub.status.idle":"2025-12-13T08:27:09.221803Z","shell.execute_reply.started":"2025-12-13T08:27:00.200615Z","shell.execute_reply":"2025-12-13T08:27:09.221052Z"}},"outputs":[{"name":"stdout","text":"Train label counts (before sampler): {0: 708, 2: 107, 3: 228, 5: 75, 6: 47, 1: 115, 4: 18}\nSampler-sampled label counts (approx over ~30 batches): {3: 145, 2: 150, 0: 129, 6: 146, 4: 144, 1: 161, 5: 117}\ntrain_loader replaced with WeightedRandomSampler. num_samples per epoch: 1298\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# === Training run using WeightedRandomSampler (5 epochs) ===\n# - Detects sampler on train_loader and chooses unweighted CE automatically.\n# - Prints per-epoch summary and per-class metrics.\n# - Safe: will use existing model if present, otherwise instantiates vit_base_patch16_224 pretrained and trains.\n\nimport os, time, math, json\nimport numpy as np\nfrom collections import Counter\nimport torch, torch.nn as nn, torch.optim as optim\nimport timm\nfrom sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ---------- CFG ----------\nEPOCHS = 5\nLR = 1e-4\nWD = 1e-2\nACCUM_STEPS = 1\nGRAD_CLIP = 1.0\nWARMUP_PCT = 0.05\nSAVE_DIR = \"/content/checkpoints_sampler\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ---------- Model: reuse if exists, else create ----------\nif 'model' in globals() and getattr(globals()['model'], 'parameters', None) is not None:\n    print(\"Reusing existing model in workspace.\")\n    model = globals()['model']\nelse:\n    print(\"No model found in workspace — creating fresh model.\")\n    model_name = globals().get('CFG',{}).get('model_name','vit_base_patch16_224')\n    num_classes = len(json.load(open(\"/content/class_to_idx.json\")))\n    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\nmodel = model.to(device)\n\n# ---------- Criterion: choose automatically ----------\n# If train_loader has a sampler (oversampling), use unweighted CE; otherwise, use precomputed `criterion` if present.\nuse_weighted = False\ntry:\n    sampler_present = getattr(train_loader, 'sampler', None) is not None\n    if sampler_present:\n        print(\"Detected sampler on train_loader -> using UNWEIGHTED CrossEntropyLoss (no class weights).\")\n        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    else:\n        if 'criterion' in globals():\n            print(\"No sampler detected -> using existing `criterion` from workspace (likely weighted).\")\n            criterion = globals()['criterion']\n        else:\n            criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\nexcept Exception:\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\ncriterion = criterion.to(device)\n\n# ---------- Optimizer, scheduler, scaler ----------\noptimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n\nsteps_per_epoch = math.ceil(len(train_loader) / 1.0)\ntotal_steps = steps_per_epoch * EPOCHS\nwarmup_steps = int(total_steps * WARMUP_PCT)\n\ndef lr_lambda(step):\n    if step < warmup_steps:\n        return float(step) / max(1, warmup_steps)\n    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n    return 0.5 * (1.0 + math.cos(math.pi * progress))\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\nscaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n\n# ---------- Helpers: validate (returns per-class arrays and prints table) ----------\nwith open(\"/content/class_to_idx.json\",\"r\") as f:\n    class_to_idx = json.load(f)\nidx_to_class = {int(v): k for k,v in class_to_idx.items()}\n\ndef validate_and_report(model, loader, criterion):\n    model.eval()\n    losses = []\n    all_logits = []\n    all_labels = []\n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs = imgs.to(device)\n            labels = labels.to(device)\n            out = model(imgs)\n            loss = criterion(out, labels)\n            losses.append(loss.item() * imgs.size(0))\n            probs = torch.softmax(out, dim=1).cpu().numpy()\n            all_logits.append(probs)\n            all_labels.append(labels.cpu().numpy())\n    if len(all_labels) == 0:\n        return {}, {}\n    all_logits = np.concatenate(all_logits, axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n    preds = all_logits.argmax(axis=1)\n    avg_loss = sum(losses) / all_labels.shape[0]\n    acc = (preds == all_labels).mean()\n    try:\n        macro_auc = roc_auc_score(all_labels, all_logits, multi_class='ovr')\n    except Exception:\n        macro_auc = float('nan')\n\n    # per-class PRF and AUC\n    prec, rec, f1, support = precision_recall_fscore_support(all_labels, preds, labels=list(range(all_logits.shape[1])), zero_division=0)\n    per_class = []\n    for i in range(all_logits.shape[1]):\n        try:\n            y_true = (all_labels == i).astype(int)\n            auc = roc_auc_score(y_true, all_logits[:, i])\n        except Exception:\n            auc = float('nan')\n        per_class.append({\n            \"class_idx\": i, \"class_name\": idx_to_class[i],\n            \"support\": int(support[i]), \"precision\": float(prec[i]),\n            \"recall\": float(rec[i]), \"f1\": float(f1[i]), \"auc\": float(auc)\n        })\n\n    cm = confusion_matrix(all_labels, preds, labels=list(range(all_logits.shape[1])))\n    metrics = {\"loss\": avg_loss, \"acc\": acc, \"macro_auc\": macro_auc, \"confusion_matrix\": cm, \"per_class\": per_class}\n    return metrics, (all_labels, all_logits)\n\n# ---------- Training loop ----------\nbest_auc = -1.0\nglobal_step = 0\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    epoch_loss = 0.0\n    correct = 0\n    total = 0\n    t0 = time.time()\n    optimizer.zero_grad()\n    for step, (imgs, labels) in enumerate(train_loader):\n        imgs = imgs.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n            out = model(imgs)\n            loss = criterion(out, labels) / ACCUM_STEPS\n        scaler.scale(loss).backward()\n        if (step + 1) % ACCUM_STEPS == 0 or (step + 1) == len(train_loader):\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n            global_step += 1\n        epoch_loss += loss.item() * ACCUM_STEPS * imgs.size(0)\n        preds = out.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n\n    train_loss = epoch_loss / total if total>0 else float('nan')\n    train_acc = correct / total if total>0 else float('nan')\n\n    # validate\n    metrics, preds_pack = validate_and_report(model, val_loader, criterion)\n    print(f\"\\nEpoch {epoch} | train_loss {train_loss:.4f} train_acc {train_acc:.4f} | val_loss {metrics['loss']:.4f} val_acc {metrics['acc']:.4f} val_auc {metrics['macro_auc']:.4f} | epoch_time {time.time()-t0:.1f}s\")\n    print(\"Confusion matrix:\")\n    print(metrics['confusion_matrix'])\n\n    # print per-class table\n    print(\"\\nPer-class results:\")\n    print(f\"{'class':>8} | {'supp':>4} | {'prec':>5} | {'rec':>5} | {'f1':>5} | {'auc':>5}\")\n    print(\"-\"*50)\n    for row in metrics['per_class']:\n        print(f\"{row['class_name']:>8} | {row['support']:4d} | {row['precision']:5.3f} | {row['recall']:5.3f} | {row['f1']:5.3f} | {row['auc']:5.3f}\")\n\n    # save checkpoint\n    ckpt = {'epoch': epoch, 'model_state': model.state_dict(), 'optim_state': optimizer.state_dict(), 'val_auc': metrics['macro_auc']}\n    torch.save(ckpt, os.path.join(SAVE_DIR, f\"epoch{epoch}_sampler.pth\"))\n    if not (isinstance(metrics['macro_auc'], float) and math.isnan(metrics['macro_auc'])) and metrics['macro_auc'] > best_auc:\n        best_auc = metrics['macro_auc']\n        torch.save(ckpt, os.path.join(SAVE_DIR, f\"best_sampler.pth\"))\n        print(\"Saved new best checkpoint.\")\n\nprint(f\"\\nDone. Best val AUC in this run: {best_auc:.4f}. Checkpoints: {SAVE_DIR}\")\n# expose model and latest preds for inspection\nglobals()['model'] = model\nglobals()['val_all'] = preds_pack\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0lL1ZVCnklY9","outputId":"9e96ed72-efd0-476d-80b7-c80d113468c7","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:34:32.778394Z","iopub.execute_input":"2025-12-13T08:34:32.779175Z","iopub.status.idle":"2025-12-13T08:37:19.747095Z","shell.execute_reply.started":"2025-12-13T08:34:32.779142Z","shell.execute_reply":"2025-12-13T08:37:19.745759Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nReusing existing model in workspace.\nDetected sampler on train_loader -> using UNWEIGHTED CrossEntropyLoss (no class weights).\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_333/4111739862.py:72: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n/tmp/ipykernel_333/4111739862.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 | train_loss 0.6059 train_acc 0.9307 | val_loss 0.7612 val_acc 0.8656 val_auc 0.9540 | epoch_time 30.1s\nConfusion matrix:\n[[169   0   1   6   0   1   0]\n [  0  25   0   0   0   2   1]\n [  3   4  13   2   2   2   0]\n [  4   0   1  49   0   2   0]\n [  0   0   0   0   4   0   0]\n [  1   5   0   3   0   9   0]\n [  1   0   0   1   0   1   8]]\n\nPer-class results:\n   class | supp |  prec |   rec |    f1 |   auc\n--------------------------------------------------\n     AMD |  177 | 0.949 | 0.955 | 0.952 | 0.982\n     DME |   28 | 0.735 | 0.893 | 0.806 | 0.987\n     ERM |   26 | 0.867 | 0.500 | 0.634 | 0.963\n      NO |   56 | 0.803 | 0.875 | 0.838 | 0.979\n     RAO |    4 | 0.667 | 1.000 | 0.800 | 1.000\n     RVO |   18 | 0.529 | 0.500 | 0.514 | 0.875\n     VID |   11 | 0.889 | 0.727 | 0.800 | 0.891\nSaved new best checkpoint.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_333/4111739862.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 | train_loss 0.5520 train_acc 0.9584 | val_loss 0.9719 val_acc 0.8219 val_auc 0.9704 | epoch_time 30.4s\nConfusion matrix:\n[[148   1  18   5   0   5   0]\n [  0  23   3   0   0   1   1]\n [  0   0  26   0   0   0   0]\n [  1   0  13  41   0   1   0]\n [  0   0   0   0   4   0   0]\n [  0   2   1   3   0  12   0]\n [  1   0   0   1   0   0   9]]\n\nPer-class results:\n   class | supp |  prec |   rec |    f1 |   auc\n--------------------------------------------------\n     AMD |  177 | 0.987 | 0.836 | 0.905 | 0.968\n     DME |   28 | 0.885 | 0.821 | 0.852 | 0.988\n     ERM |   26 | 0.426 | 1.000 | 0.598 | 0.980\n      NO |   56 | 0.820 | 0.732 | 0.774 | 0.915\n     RAO |    4 | 1.000 | 1.000 | 1.000 | 1.000\n     RVO |   18 | 0.632 | 0.667 | 0.649 | 0.952\n     VID |   11 | 0.900 | 0.818 | 0.857 | 0.989\nSaved new best checkpoint.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_333/4111739862.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 | train_loss 0.5444 train_acc 0.9607 | val_loss 0.7036 val_acc 0.9000 val_auc 0.9869 | epoch_time 31.1s\nConfusion matrix:\n[[163   0   1  11   0   1   1]\n [  0  23   1   0   0   4   0]\n [  0   0  25   1   0   0   0]\n [  0   0   4  52   0   0   0]\n [  0   0   0   0   4   0   0]\n [  1   2   0   3   0  12   0]\n [  1   0   0   1   0   0   9]]\n\nPer-class results:\n   class | supp |  prec |   rec |    f1 |   auc\n--------------------------------------------------\n     AMD |  177 | 0.988 | 0.921 | 0.953 | 0.978\n     DME |   28 | 0.920 | 0.821 | 0.868 | 0.995\n     ERM |   26 | 0.806 | 0.962 | 0.877 | 0.997\n      NO |   56 | 0.765 | 0.929 | 0.839 | 0.981\n     RAO |    4 | 1.000 | 1.000 | 1.000 | 1.000\n     RVO |   18 | 0.706 | 0.667 | 0.686 | 0.967\n     VID |   11 | 0.900 | 0.818 | 0.857 | 0.991\nSaved new best checkpoint.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_333/4111739862.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 | train_loss 0.4689 train_acc 0.9923 | val_loss 0.6757 val_acc 0.9031 val_auc 0.9904 | epoch_time 30.8s\nConfusion matrix:\n[[163   0   1  10   0   2   1]\n [  0  24   1   0   0   1   2]\n [  0   1  23   2   0   0   0]\n [  1   0   1  54   0   0   0]\n [  0   0   0   0   4   0   0]\n [  0   3   0   3   0  12   0]\n [  1   0   0   1   0   0   9]]\n\nPer-class results:\n   class | supp |  prec |   rec |    f1 |   auc\n--------------------------------------------------\n     AMD |  177 | 0.988 | 0.921 | 0.953 | 0.986\n     DME |   28 | 0.857 | 0.857 | 0.857 | 0.995\n     ERM |   26 | 0.885 | 0.885 | 0.885 | 0.996\n      NO |   56 | 0.771 | 0.964 | 0.857 | 0.984\n     RAO |    4 | 1.000 | 1.000 | 1.000 | 1.000\n     RVO |   18 | 0.800 | 0.667 | 0.727 | 0.975\n     VID |   11 | 0.750 | 0.818 | 0.783 | 0.997\nSaved new best checkpoint.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_333/4111739862.py:138: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_333/4111739862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_and_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch} | train_loss {train_loss:.4f} train_acc {train_acc:.4f} | val_loss {metrics['loss']:.4f} val_acc {metrics['acc']:.4f} val_auc {metrics['macro_auc']:.4f} | epoch_time {time.time()-t0:.1f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion matrix:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_333/4111739862.py\u001b[0m in \u001b[0;36mvalidate_and_report\u001b[0;34m(model, loader, criterion)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_333/784118655.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2299\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mResampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"# === Focused training (CLEAN, portable, no Colab assumptions) ===\n\nimport os, time, math\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\n\n# ---------------- CONFIG ----------------\nCFG = {\n    \"model_name\": \"deit_base_patch16_224\",\n    \"pretrained\": True,\n\n    \"max_epochs\": 40,\n    \"target_val_acc\": 0.95,\n\n    \"lr\": 1e-4,\n    \"weight_decay\": 1e-2,\n\n    \"accum_steps\": 8,\n    \"grad_clip\": 0.5,\n    \"warmup_pct\": 0.06,\n\n    \"num_workers\": 0,\n    \"label_smoothing\": 0.1,\n\n    \"input_size\": 384,\n    \"save_dir\": \"/kaggle/working/checkpoints\"\n}\n\nos.makedirs(CFG[\"save_dir\"], exist_ok=True)\n\n# ---------------- Device ----------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ---------------- Infer num_classes robustly ----------------\ndef infer_num_classes(ds):\n    labels = set()\n    for i in range(len(ds)):\n        _, y = ds[i]\n        labels.add(int(y))\n    return len(labels)\n\nnum_classes = infer_num_classes(train_loader.dataset)\nprint(\"Detected num_classes:\", num_classes)\n\n# ---------------- Model (safe reuse) ----------------\nreuse_ok = (\n    \"model\" in globals()\n    and isinstance(globals()[\"model\"], torch.nn.Module)\n    and getattr(globals()[\"model\"], \"num_classes\", None) == num_classes\n)\n\nif reuse_ok:\n    model = globals()[\"model\"]\n    print(\"Reusing existing compatible model.\")\nelse:\n    print(\"Creating new model:\", CFG[\"model_name\"])\n    model = timm.create_model(\n        CFG[\"model_name\"],\n        pretrained=CFG[\"pretrained\"],\n        num_classes=num_classes\n    )\n\nmodel = model.to(device)\n\n# ---------------- Rebuild DataLoaders (workers=0 safe) ----------------\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    train_loader.dataset,\n    batch_size=train_loader.batch_size,\n    sampler=train_loader.sampler if hasattr(train_loader, \"sampler\") else None,\n    shuffle=False if hasattr(train_loader, \"sampler\") else True,\n    num_workers=0,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_loader.dataset,\n    batch_size=val_loader.batch_size,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=True\n)\n\n# ---------------- Criterion ----------------\n# If sampler is used, DO NOT weight loss again\ncriterion = nn.CrossEntropyLoss(label_smoothing=CFG[\"label_smoothing\"]).to(device)\n\n# ---------------- Optimizer / Scheduler / AMP ----------------\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=CFG[\"lr\"],\n    weight_decay=CFG[\"weight_decay\"]\n)\n\nsteps_per_epoch = len(train_loader)\ntotal_steps = steps_per_epoch * CFG[\"max_epochs\"]\nwarmup_steps = int(total_steps * CFG[\"warmup_pct\"])\n\ndef lr_lambda(step):\n    if step < warmup_steps:\n        return step / max(1, warmup_steps)\n    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n    return 0.5 * (1.0 + math.cos(math.pi * progress))\n\nscheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\nscaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n\n# ---------------- Quick validation ----------------\n@torch.no_grad()\ndef quick_validate(model, loader):\n    model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    for imgs, labels in loader:\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n        out = model(imgs)\n        loss = criterion(out, labels)\n        total_loss += loss.item() * imgs.size(0)\n        correct += (out.argmax(1) == labels).sum().item()\n        total += imgs.size(0)\n    return total_loss / total, correct / total\n\n# ---------------- Training loop ----------------\nbest_val_acc = -1.0\nbest_epoch = -1\n\nfor epoch in range(1, CFG[\"max_epochs\"] + 1):\n    model.train()\n    optimizer.zero_grad()\n\n    epoch_loss, correct, total = 0.0, 0, 0\n    t0 = time.time()\n\n    for step, (imgs, labels) in enumerate(train_loader):\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n            out = model(imgs)\n            loss = criterion(out, labels) / CFG[\"accum_steps\"]\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % CFG[\"accum_steps\"] == 0 or (step + 1) == len(train_loader):\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CFG[\"grad_clip\"])\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n\n        epoch_loss += loss.item() * CFG[\"accum_steps\"] * imgs.size(0)\n        correct += (out.argmax(1) == labels).sum().item()\n        total += imgs.size(0)\n\n    train_loss = epoch_loss / total\n    train_acc = correct / total\n\n    val_loss, val_acc = quick_validate(model, val_loader)\n    dt = time.time() - t0\n\n    print(\n        f\"Epoch {epoch}/{CFG['max_epochs']} | \"\n        f\"time {dt:.1f}s | \"\n        f\"train_loss {train_loss:.4f} train_acc {train_acc:.4f} | \"\n        f\"val_loss {val_loss:.4f} val_acc {val_acc:.4f}\"\n    )\n\n    # Save last\n    torch.save(\n        {\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"optim_state\": optimizer.state_dict(),\n            \"val_acc\": val_acc,\n            \"cfg\": CFG,\n        },\n        os.path.join(CFG[\"save_dir\"], f\"last_{CFG['model_name']}.pth\")\n    )\n\n    # Save best\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_epoch = epoch\n        best_path = os.path.join(CFG[\"save_dir\"], f\"best_{CFG['model_name']}.pth\")\n        torch.save(\n            {\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optim_state\": optimizer.state_dict(),\n                \"val_acc\": val_acc,\n                \"cfg\": CFG,\n            },\n            best_path\n        )\n        print(\"Saved new best checkpoint.\")\n\n    # Early stop\n    if val_acc >= CFG[\"target_val_acc\"]:\n        print(f\"Target val_acc {CFG['target_val_acc']:.2f} reached at epoch {epoch}. Stopping.\")\n        break\n\nglobals()[\"model\"] = model\nprint(f\"Finished. Best val_acc: {best_val_acc:.4f} at epoch {best_epoch}\")\nprint(f\"Checkpoints saved in: {CFG['save_dir']}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYsxK1NLnXsz","outputId":"933a7ce4-116f-496e-f7db-614ca398ca58","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:37:19.749363Z","iopub.status.idle":"2025-12-13T08:37:19.749698Z","shell.execute_reply.started":"2025-12-13T08:37:19.749495Z","shell.execute_reply":"2025-12-13T08:37:19.749525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === CELL 1: Evaluation & Metrics ===\nimport os, json, numpy as np, torch\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ---------- class names ----------\nif 'class_names' in globals() and isinstance(class_names, (list,tuple)):\n    names = list(class_names)\nelse:\n    try:\n        with open(\"/content/class_to_idx.json\",\"r\") as f:\n            ct = json.load(f)\n        names = [None]*len(ct)\n        for k,v in ct.items():\n            names[int(v)] = k\n    except Exception:\n        names = train_loader.dataset.classes\n\nnum_classes = len(names)\nprint(\"Classes:\", names)\n\n# ---------- load best checkpoint ----------\nckpt_path = os.path.join(\"/kaggle/working/checkpoints\", f\"best_{CFG['model_name']}.pth\")\nckpt = torch.load(ckpt_path, map_location=device)\nmodel.load_state_dict(ckpt[\"model_state\"], strict=False)\nmodel.to(device).eval()\n\nprint(f\"Loaded best checkpoint | epoch={ckpt.get('epoch')} val_acc={ckpt.get('val_acc')}\")\n\n# ---------- inference ----------\ny_true, y_pred, probs_list = [], [], []\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits = model(imgs)\n        probs = torch.softmax(logits, dim=1).cpu().numpy()\n        preds = logits.argmax(dim=1).cpu().numpy()\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(preds)\n        probs_list.append(probs)\n\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\nprobs_all = np.vstack(probs_list)\n\n# ---------- metrics ----------\nacc = (y_true == y_pred).mean()\nprint(f\"\\nValidation accuracy: {acc:.4f}\\n\")\n\nprecision, recall, f1, support = precision_recall_fscore_support(\n    y_true, y_pred, labels=range(num_classes), zero_division=0\n)\n\nprint(classification_report(y_true, y_pred, target_names=names, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:30:51.508169Z","iopub.status.idle":"2025-12-13T08:30:51.508530Z","shell.execute_reply.started":"2025-12-13T08:30:51.508365Z","shell.execute_reply":"2025-12-13T08:30:51.508383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === CELL 2: Confusion Matrix ===\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(7,6))\nsns.heatmap(\n    cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n    xticklabels=names, yticklabels=names\n)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:30:51.509718Z","iopub.status.idle":"2025-12-13T08:30:51.509950Z","shell.execute_reply.started":"2025-12-13T08:30:51.509842Z","shell.execute_reply":"2025-12-13T08:30:51.509852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === CELL 3: Training Dynamics & Transformer Plots ===\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# -------------------------------------------------\n# Plot 1: Validation Accuracy vs Epoch\n# -------------------------------------------------\nepochs, val_accuracies = None, None\n\nif \"history\" in globals():\n    epochs = [h[\"epoch\"] for h in history]\n    val_accuracies = [h[\"val_acc\"] for h in history]\nelse:\n    try:\n        val_accuracies = np.load(\"val_accuracies.npy\")\n        epochs = np.arange(1, len(val_accuracies) + 1)\n    except Exception:\n        print(\"No validation history found — skipping val acc curve.\")\n\nif epochs is not None:\n    plt.figure(figsize=(6,4))\n    plt.plot(epochs, val_accuracies, marker=\"o\")\n    plt.axhline(0.95, color=\"red\", linestyle=\"--\", label=\"Target 95%\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Validation Accuracy\")\n    plt.title(\"Validation Accuracy vs Epoch\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n# -------------------------------------------------\n# Plot 2: Per-class Recall (MOST IMPORTANT)\n# -------------------------------------------------\nplt.figure(figsize=(8,4))\nplt.bar(names, recall)\nplt.ylabel(\"Recall\")\nplt.title(\"Per-class Recall (Sensitivity)\")\nplt.ylim(0, 1.05)\nplt.grid(axis=\"y\")\nplt.show()\n\n# -------------------------------------------------\n# Plot 3: Transformer Confidence Histogram\n# -------------------------------------------------\nmax_conf = probs_all.max(axis=1)\n\nplt.figure(figsize=(6,4))\nplt.hist(max_conf, bins=20, edgecolor=\"black\")\nplt.xlabel(\"Max Softmax Probability\")\nplt.ylabel(\"Number of Samples\")\nplt.title(\"Prediction Confidence Distribution\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T08:30:51.511457Z","iopub.status.idle":"2025-12-13T08:30:51.511713Z","shell.execute_reply.started":"2025-12-13T08:30:51.511594Z","shell.execute_reply":"2025-12-13T08:30:51.511605Z"}},"outputs":[],"execution_count":null}]}